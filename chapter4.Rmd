
## Clustering and classification

```{r}
date()
```
**FIRST, I created a new R Markdown file**, named it as 'chapter4.Rmd', and saved it to my folder. Then, I included the new file as a child file in my 'index.Rmd' file.

**SECOND, I loaded the Boston data set** from the MASS package and explored its structure and dimensions.

```{r}
library(MASS)
data("Boston")
str(Boston)
summary(Boston)
```

The **Boston** data consist of 506 rows and 14 columns. It describes demographic, socioeconomic, and environmental characteristics of towns.

References to the data source are as follows:

- Harrison, D. and Rubinfeld, D.L. (1978) Hedonic prices and the demand for clean air. J. Environ. Economics and Management 5, 81â€“102.
- Belsley D.A., Kuh, E. and Welsch, R.E. (1980) Regression Diagnostics. Identifying Influential Data and Sources of Collinearity. New York: Wiley.

**THIRD, I graphically and numerically described the Boston data** variables and relationships across them.

```{r echo=FALSE}
library(corrplot)
library(tidyr)
library(MASS)
data("Boston")
par(mfrow = c(2,4))
hist(Boston$crim, main="crim", xlab="")
hist(Boston$zn, main="zn", xlab="")
hist(Boston$indus, main="indus", xlab="")
hist(Boston$chas, main="chas", xlab="")
hist(Boston$nox, main="nox", xlab="")
hist(Boston$rm, main="rm", xlab="")
hist(Boston$age, main="age", xlab="")
```
```{r echo=FALSE}
library(corrplot)
library(tidyr)
library(MASS)
data("Boston")
par(mfrow = c(2,4))
hist(Boston$dis, main="dis", xlab="")
hist(Boston$rad, main="rad", xlab="")
hist(Boston$tax, main="tax", xlab="")
hist(Boston$ptratio, main="ptratio", xlab="")
hist(Boston$black, main="black", xlab="")
hist(Boston$lstat, main="lstat", xlab="")
hist(Boston$medv, main="medv", xlab="")
```

Most **Boston** data variables are not normally distributed. By eye, only **rm** i.e. the average number of rooms per dwelling appears to follow a normal distribution.

```{r echo=FALSE}
library(corrplot)
library(tidyr)
library(MASS)
data("Boston")
cor_matrix<-cor(Boston) %>% round(digits = 2)
cor_matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos="b", tl.pos="d", tl.cex = 0.6)
```

There are some strong correlations (*r* >0.70) across the **Boston** data variables. The strong positive correlations are **tax** x **rad** (*r* = 0.91), **nox** x **indus** (*r* = 0.76), **nox** x **age** (*r* = 0.73), and **tax** x **indus** (*r* = 0.72). Correspondingly, the strong negative correlations are **nox** x **dis** (*r* = -0.77), **age** x **dis** (*r* = -0.75), **medv** x **lstat** (*r* = -0.74), and **dis** x **indus** (*r* = -0.71).

**FOURTH, I standardized the Boston data set, modified the crime rate variable, and divided the data set to train and test sets.**

```{r}
library(MASS)
data("Boston")
boston_scaled <- scale(Boston)
summary(boston_scaled)
```

Standardization re-scaled the variables. After the standardization, respective means and standard deviations of the variables are 0 and 1.

```{r}
boston_scaled <- as.data.frame(boston_scaled)
bins <- quantile(boston_scaled$crim)
bins
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))
table(crime)
boston_scaled <- dplyr::select(boston_scaled, -crim)
boston_scaled <- data.frame(boston_scaled, crime)
```

```{r}
n <- nrow(boston_scaled)
ind <- sample(n,  size = n * 0.8)
train <- boston_scaled[ind,]
test <- boston_scaled[-ind,]
```

**FIFTH, I fitted the linear discriminant analysis on the Boston train set** using the categorical crime rate as the target variable and all other variables as predictors.

```{r echo=FALSE}
n <- nrow(boston_scaled)
ind <- sample(n,  size = n * 0.8)
train <- boston_scaled[ind,]
test <- boston_scaled[-ind,]
lda.fit <- lda(crime ~ ., data = train)
lda.fit
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "orange", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}
classes <- as.numeric(train$crime)
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)
```

**SIXTH, I predicted the crime rate classes in the Boston test set with the LDA model.**

```{r echo=FALSE}
n <- nrow(boston_scaled)
ind <- sample(n,  size = n * 0.8)
train <- boston_scaled[ind,]
test <- boston_scaled[-ind,]
correct_classes <- test$crime
test <- dplyr::select(test, -crime)
lda.pred <- predict(lda.fit, newdata = test)
table(correct = correct_classes, predicted = lda.pred$class)
```

The **prediction error** of the model ranged from 18% to 26%. The model was good in recognizing "low" (low + low_med) and "high" (med_high + high) categories. With respect to this dichotomized categorization, the prediction error was c. 10%.

**SEVENTH, I run the k-means algorithm on the Boston data set.**

```{r}
library(ggplot2)
library(MASS)
data("Boston")
boston_scaled <- scale(Boston)
boston_scaled <- as.data.frame(boston_scaled)
dist_eu <- dist(boston_scaled)
summary(dist_eu)
dist_man <- dist(boston_scaled, method = 'manhattan')
summary(dist_man)
km <-kmeans(boston_scaled, centers = 4)
pairs(boston_scaled[1:6], col = km$cluster)
set.seed(123)
k_max <- 10
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})
qplot(x = 1:k_max, y = twcss, geom = 'line')
km <-kmeans(boston_scaled, centers = 2)
pairs(boston_scaled[1:6], col = km$cluster)
```

Based on the total of within cluster sum of squares (WCSS), I chose 2 as the optimal number of clusters. Actually, the two clusters solution did not much differ from the four clusters solution, but they both highlighted the same variables, such as **crim** and **zn**, as the most representative factors.

**EIGHT, I performed the BONUS task.**

```{r}
library(MASS)
data("Boston")
boston_scaled <- scale(Boston)
boston_scaled <- as.data.frame(boston_scaled)
km <-kmeans(boston_scaled, centers = 4)
lda.fit2 <- lda(km$cluster ~ ., data = boston_scaled)
lda.fit2
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "orange", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}
classes <- as.numeric(km$cluster)
plot(lda.fit2, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit2, myscale = 2)
```

In this four clusters solution, the **Charles River** dummy variable (= 1 if tract bounds river; 0 otherwise) was the main driver of clustering. It separated Cluster 1 from other clusters.

OBS! I did not carried out the SUPER-BONUS task.